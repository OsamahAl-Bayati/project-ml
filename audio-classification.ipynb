{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Epoch: 0, Loss: 1.88, Accuracy: 0.33\n",
      "Epoch: 1, Loss: 1.51, Accuracy: 0.48\n",
      "Training Complete\n",
      "Accuracy: 0.51, Total items: 1746\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchaudio import transforms\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "def install_libraries():\n",
    "    !pip install pandas torch torchaudio torchvision\n",
    "\n",
    "def prepare_metadata(directory_path: str) -> pd.DataFrame:\n",
    "    metadata_file = Path(directory_path) / 'metadata' / 'UrbanSound8K.csv'\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "    metadata['path'] = metadata['slice_file_name'].apply(lambda x: f\"/fold{metadata.loc[metadata['slice_file_name'] == x, 'fold'].iloc[0]}/{x}\")\n",
    "    metadata = metadata[['path', 'classID']]\n",
    "    metadata = metadata.rename(columns={'path': 'relative_path'})\n",
    "    return metadata\n",
    "\n",
    "class AudioAugment:\n",
    "\n",
    "    # Insert an audio file. Return the signal as a tensor as well as the sampling rate.\n",
    "    @staticmethod\n",
    "    def open(wav_file):\n",
    "        sig, sampling_rate = torchaudio.load(wav_file)\n",
    "        return (sig, sampling_rate)\n",
    "\n",
    "    # Convert the audio provided to the desired number of channels\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sampling_rate = aud\n",
    "\n",
    "        if sig.shape[0] == new_channel:\n",
    "            return aud\n",
    "\n",
    "        if new_channel == 1:\n",
    "            # Convert first channel to mono.\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            # Convert from mono to stereo.\n",
    "            resig = torch.cat([sig, sig])\n",
    "\n",
    "        return (resig, sampling_rate)\n",
    "\n",
    "    # Because Resample only applies to one channel, we resample one channel at a time.\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sampling_rate = aud\n",
    "\n",
    "        if sampling_rate == newsr:\n",
    "            return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "        # First channel resampling\n",
    "        resig = torchaudio.transforms.Resample(sampling_rate, newsr)(sig[:1, :])\n",
    "        if num_channels > 1:\n",
    "            # Resample the second channel and merge it with the first.\n",
    "            retwo = torchaudio.transforms.Resample(sampling_rate, newsr)(sig[1:, :])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return (resig, newsr)\n",
    "\n",
    "    # Truncate or Pad the signal to a fixed length in milliseconds ('maximum audio length').\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, maximum_audio_length):\n",
    "        sig, sampling_rate = aud\n",
    "        num_rows, input_signal_length = sig.shape\n",
    "        maximum_length = sampling_rate // 1000 * maximum_audio_length\n",
    "\n",
    "        if input_signal_length > maximum_length:\n",
    "            # Reduce the signal to the specified length.\n",
    "            sig = sig[:, :maximum_length]\n",
    "\n",
    "        elif input_signal_length < maximum_length:\n",
    "            # Padding length to be added at the beginning and end of the signal\n",
    "            padding_begin_length = random.randint(0, maximum_length - input_signal_length)\n",
    "            padding_end_length = maximum_length - input_signal_length - padding_begin_length\n",
    "\n",
    "            # Pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, padding_begin_length))\n",
    "            pad_end = torch.zeros((num_rows, padding_end_length))\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "\n",
    "        return (sig, sampling_rate)\n",
    "\n",
    "    # Shifts the signal by a percentage to the left or right. End values are 'wrapped around' to the beginning of the transformed signal.\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sampling_rate = aud\n",
    "        _, input_signal_length = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * input_signal_length)\n",
    "        return (sig.roll(shift_amt), sampling_rate)\n",
    "\n",
    "    # Create a Spectrogram\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sampling_rate = aud\n",
    "        top_db = 80\n",
    "\n",
    "        # The shape of spec is [channel, n mels, time], where channel is mono, stereo, and so on.\n",
    "        spec = transforms.MelSpectrogram(\n",
    "            sampling_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels\n",
    "        )(sig)\n",
    "\n",
    "        # Decibel conversion\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec\n",
    "\n",
    "    # Mask out some sections of the Spectrogram in both the frequency dimension (horizontal bars) and the time dimension (vertical bars) to prevent overfitting and help the model generalize better. The mean value is used to replace the masked sections.\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(\n",
    "                aug_spec, mask_value\n",
    "            )\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir):\n",
    "        self.df = df\n",
    "        self.audio_dir = str(audio_dir)\n",
    "        self.length = 4000\n",
    "        self.rate = 44100\n",
    "        self.channels = 2\n",
    "        self.shift_percentage = 0.4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_filepath = self.audio_dir + self.df.loc[index, \"relative_path\"]\n",
    "        label = self.df.loc[index, \"classID\"]\n",
    "\n",
    "        audio = AudioAugment.open(audio_filepath)\n",
    "        resampled_audio = AudioAugment.resample(audio, self.rate)\n",
    "        rechanneled_audio = AudioAugment.rechannel(resampled_audio, self.channels)\n",
    "\n",
    "        padded_audio = AudioAugment.pad_trunc(rechanneled_audio, self.length)\n",
    "        shifted_audio = AudioAugment.time_shift(padded_audio, self.shift_percentage)\n",
    "        spectrogram = AudioAugment.spectro_gram(shifted_audio, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        augmented_spectrogram = AudioAugment.spectro_augment(\n",
    "            spectrogram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2\n",
    "        )\n",
    "\n",
    "        return augmented_spectrogram, label\n",
    "\n",
    "def generate_data_loaders(df, a_path):\n",
    "    dataset = AudioDataset(df, a_path)\n",
    "\n",
    "    total_items = len(dataset)\n",
    "    total_items = len(dataset)\n",
    "    train_items = round(total_items * 0.8)\n",
    "    val_items = total_items - train_items\n",
    "    training_set, validation_set = random_split(dataset, [train_items, val_items])\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(training_set, batch_size=16, shuffle=True)\n",
    "    val_data_loader = torch.utils.data.DataLoader(validation_set, batch_size=16, shuffle=False)\n",
    "\n",
    "    return train_data_loader, val_data_loader   \n",
    "\n",
    "class SoundClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoundClassifier, self).__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            self._conv_block(2, 8, 5, 2, 2),\n",
    "            self._conv_block(8, 16, 3, 2, 1),\n",
    "            self._conv_block(16, 32, 3, 2, 1),\n",
    "            self._conv_block(32, 64, 3, 2, 1)\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        relu = nn.ReLU()\n",
    "        bn = nn.BatchNorm2d(out_channels)\n",
    "        init.kaiming_normal_(conv.weight, a=0.1)\n",
    "        conv.bias.data.zero_()\n",
    "        return nn.Sequential(conv, relu, bn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def fit_model(mdl, trn_ldr, n_epochs, dev):\n",
    "    # Set up necessary components\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(mdl.parameters(), lr=1e-3)\n",
    "    sched = OneCycleLR(opt, max_lr=1e-3, steps_per_epoch=len(trn_ldr), epochs=n_epochs, anneal_strategy=\"linear\")\n",
    "\n",
    "    # Train for each epoch\n",
    "    for ep in range(n_epochs):\n",
    "        total_loss, correct, total_preds = 0.0, 0, 0\n",
    "\n",
    "        # Process each batch in the training set\n",
    "        for idx, batch_data in enumerate(trn_ldr):\n",
    "            # Move input and target data to the device\n",
    "            inputs, batch_labels = batch_data[0].to(dev), batch_data[1].to(dev)\n",
    "\n",
    "            # Normalize input data\n",
    "            mean, std = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - mean) / std\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Perform forward pass, calculate loss, and update weights\n",
    "            preds = mdl(inputs)\n",
    "            loss = loss_fn(preds, batch_labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "\n",
    "            # Update loss and accuracy\n",
    "            total_loss += loss.item()\n",
    "            _, predicted_class = torch.max(preds, 1)\n",
    "            correct += (predicted_class == batch_labels).sum().item()\n",
    "            total_preds += predicted_class.shape[0]\n",
    "\n",
    "        # Print statistics for the current epoch\n",
    "        avg_loss = total_loss / len(trn_ldr)\n",
    "        acc = correct / total_preds\n",
    "        print(f\"Epoch: {ep}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}\")\n",
    "\n",
    "    print(\"Training Complete\")\n",
    "\n",
    "def test_inference(model, validation_dataloader, device):\n",
    "    number_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    # Turn off gradient updates.\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            # Place the input features and target labels on the device (GPU or CPU).\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Standardize inputs.\n",
    "            inputs = (inputs - inputs.mean()) / inputs.std()\n",
    "\n",
    "            # Get predicted outputs.\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the class with the highest predicted score.\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # The number of correct predictions.\n",
    "            number_correct += (predictions == labels).sum().item()\n",
    "\n",
    "            # The total number of examples.\n",
    "            num_examples += inputs.size(0)\n",
    "\n",
    "    accuracy = number_correct / num_examples\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Total items: {num_examples}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    install_libraries()\n",
    "\n",
    "    base_directory = Path.cwd() / 'UrbanSound8K'\n",
    "    dataframe = prepare_metadata(base_directory)\n",
    "    audio_path = base_directory / 'audio'\n",
    "    train_dataloader, validation_dataloader = generate_data_loaders(dataframe, audio_path)\n",
    "\n",
    "    audioClassifierModel = SoundClassifier()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    audioClassifierModel = audioClassifierModel.to(device)\n",
    "\n",
    "    number_of_epochs = 2\n",
    "    fit_model(audioClassifierModel, train_dataloader, number_of_epochs, device)\n",
    "    test_inference(audioClassifierModel, validation_dataloader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
