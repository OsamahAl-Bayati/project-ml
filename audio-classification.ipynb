{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Epoch: 0, Loss: 1.88, Accuracy: 0.33\n",
      "Epoch: 1, Loss: 1.54, Accuracy: 0.46\n",
      "Finished Training\n",
      "Accuracy: 0.48, Total items: 1746\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchaudio import transforms\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "\n",
    "def install_libraries():\n",
    "    !pip install pandas torch torchaudio torchvision\n",
    "\n",
    "def prepare_metadata(base_directory: Path):\n",
    "    metadata = base_directory/'metadata'/'UrbanSound8K.csv'\n",
    "    dataframe = pd.read_csv(metadata)\n",
    "    dataframe['relative_path'] = '/fold' + dataframe['fold'].astype(str) + '/' + dataframe['slice_file_name'].astype(str)\n",
    "    dataframe = dataframe[['relative_path', 'classID']]\n",
    "    return dataframe\n",
    "\n",
    "class AudioAugment:\n",
    "\n",
    "    # Insert an audio file. Return the signal as a tensor as well as the sampling rate.\n",
    "    @staticmethod\n",
    "    def open(wav_file):\n",
    "        sig, sampling_rate = torchaudio.load(wav_file)\n",
    "        return (sig, sampling_rate)\n",
    "\n",
    "    # Convert the audio provided to the desired number of channels\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sampling_rate = aud\n",
    "\n",
    "        if sig.shape[0] == new_channel:\n",
    "            return aud\n",
    "\n",
    "        if new_channel == 1:\n",
    "            # Convert first channel to mono.\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            # Convert from mono to stereo.\n",
    "            resig = torch.cat([sig, sig])\n",
    "\n",
    "        return (resig, sampling_rate)\n",
    "\n",
    "    # Because Resample only applies to one channel, we resample one channel at a time.\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sampling_rate = aud\n",
    "\n",
    "        if sampling_rate == newsr:\n",
    "            return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "        # First channel resampling\n",
    "        resig = torchaudio.transforms.Resample(sampling_rate, newsr)(sig[:1, :])\n",
    "        if num_channels > 1:\n",
    "            # Resample the second channel and merge it with the first.\n",
    "            retwo = torchaudio.transforms.Resample(sampling_rate, newsr)(sig[1:, :])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return (resig, newsr)\n",
    "\n",
    "    # Truncate or Pad the signal to a fixed length in milliseconds ('maximum audio length').\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, maximum_audio_length):\n",
    "        sig, sampling_rate = aud\n",
    "        num_rows, input_signal_length = sig.shape\n",
    "        maximum_length = sampling_rate // 1000 * maximum_audio_length\n",
    "\n",
    "        if input_signal_length > maximum_length:\n",
    "            # Reduce the signal to the specified length.\n",
    "            sig = sig[:, :maximum_length]\n",
    "\n",
    "        elif input_signal_length < maximum_length:\n",
    "            # Padding length to be added at the beginning and end of the signal\n",
    "            padding_begin_length = random.randint(0, maximum_length - input_signal_length)\n",
    "            padding_end_length = maximum_length - input_signal_length - padding_begin_length\n",
    "\n",
    "            # Pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, padding_begin_length))\n",
    "            pad_end = torch.zeros((num_rows, padding_end_length))\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "\n",
    "        return (sig, sampling_rate)\n",
    "\n",
    "    # Shifts the signal by a percentage to the left or right. End values are 'wrapped around' to the beginning of the transformed signal.\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sampling_rate = aud\n",
    "        _, input_signal_length = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * input_signal_length)\n",
    "        return (sig.roll(shift_amt), sampling_rate)\n",
    "\n",
    "    # Create a Spectrogram\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sampling_rate = aud\n",
    "        top_db = 80\n",
    "\n",
    "        # The shape of spec is [channel, n mels, time], where channel is mono, stereo, and so on.\n",
    "        spec = transforms.MelSpectrogram(\n",
    "            sampling_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels\n",
    "        )(sig)\n",
    "\n",
    "        # Decibel conversion\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec\n",
    "\n",
    "    # Mask out some sections of the Spectrogram in both the frequency dimension (horizontal bars) and the time dimension (vertical bars) to prevent overfitting and help the model generalize better. The mean value is used to replace the masked sections.\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(\n",
    "                aug_spec, mask_value\n",
    "            )\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, dataframe, audio_path):\n",
    "        self.dataframe = dataframe\n",
    "        self.audio_path = str(audio_path)\n",
    "        self.duration = 4000\n",
    "        self.sampling_rate = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "\n",
    "    # The number of items in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    # Get the i'th item in the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute audio file path - concatenate the audio directory with the relative path\n",
    "        audio_file = self.audio_path + self.dataframe.loc[idx, \"relative_path\"]\n",
    "        # Obtain the Class ID\n",
    "        class_id = self.dataframe.loc[idx, \"classID\"]\n",
    "\n",
    "        aud = AudioAugment.open(audio_file)\n",
    "        # When compared to the majority, some sounds have a higher sample rate or fewer channels. As a result, ensure that all sounds have the same number of channels and sample rate. Even if the sound duration is the same, the pad trunc will produce arrays of varying lengths unless the sample rate is the same.\n",
    "        reaud = AudioAugment.resample(aud, self.sampling_rate)\n",
    "        rechan = AudioAugment.rechannel(reaud, self.channel)\n",
    "\n",
    "        dur_aud = AudioAugment.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioAugment.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioAugment.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioAugment.spectro_augment(\n",
    "            sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2\n",
    "        )\n",
    "\n",
    "        return aug_sgram, class_id\n",
    "\n",
    "def create_data_loaders(dataframe, audio_path):\n",
    "    myds = SoundDS(dataframe, audio_path)\n",
    "\n",
    "    num_items = len(myds)\n",
    "    num_train = round(num_items * 0.8)\n",
    "    num_val = num_items - num_train\n",
    "    train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    validation_dataloader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "    return train_dataloader, validation_dataloader\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "\n",
    "    # Create the model architecture.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        convolutional_layers_list = []\n",
    "\n",
    "        # The first convolution block is made up of Relu and Batch Norm. Make use of Kaiming Initialization.\n",
    "        self.convolutional_layer1 = nn.Conv2d(2, 8, padding=(2, 2), kernel_size=(5, 5), stride=(2, 2))\n",
    "        self.rectified_linear_unit1 = nn.ReLU()\n",
    "        self.batch_normalization1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.convolutional_layer1.weight, a=0.1)\n",
    "        self.convolutional_layer1.bias.data.zero_()\n",
    "        convolutional_layers_list += [self.convolutional_layer1, self.rectified_linear_unit1, self.batch_normalization1]\n",
    "\n",
    "        # Convolution-Block number 2\n",
    "        self.convolutional_layer2 = nn.Conv2d(8, 16, padding=(1, 1), kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.rectified_linear_unit2 = nn.ReLU()\n",
    "        self.batch_normalization2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.convolutional_layer2.weight, a=0.1)\n",
    "        self.convolutional_layer2.bias.data.zero_()\n",
    "        convolutional_layers_list += [self.convolutional_layer2, self.rectified_linear_unit2, self.batch_normalization2]\n",
    "\n",
    "        # Convolution-Block number 2\n",
    "        self.convolutional_layer3 = nn.Conv2d(16, 32, padding=(1, 1), kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.rectified_linear_unit3 = nn.ReLU()\n",
    "        self.batch_normalization3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.convolutional_layer3.weight, a=0.1)\n",
    "        self.convolutional_layer3.bias.data.zero_()\n",
    "        convolutional_layers_list += [self.convolutional_layer3, self.rectified_linear_unit3, self.batch_normalization3]\n",
    "\n",
    "        # Convolution-Block number 2\n",
    "        self.convolutional_layer4 = nn.Conv2d( 32, 64, padding=(1, 1), kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.rectified_linear_unit4 = nn.ReLU()\n",
    "        self.batch_normalization4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.convolutional_layer4.weight, a=0.1)\n",
    "        self.convolutional_layer4.bias.data.zero_()\n",
    "        convolutional_layers_list += [self.convolutional_layer4, self.rectified_linear_unit4, self.batch_normalization4]\n",
    "\n",
    "        # Linear-Classifier\n",
    "        self.adaptive_average_pooling = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "        # Convolutional Blocks Should Be Wrapped\n",
    "        self.convolutional_layer_container = nn.Sequential(*convolutional_layers_list)\n",
    "\n",
    "    # Computed in the first pass\n",
    "    def forward(self, x):\n",
    "        # Execute the convolutional blocks.\n",
    "        x = self.convolutional_layer_container(x)\n",
    "\n",
    "        # Adaptive pooling and flattening for linear layer input\n",
    "        x = self.adaptive_average_pooling(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # The linear layer\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # final output result\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_dataloader, number_of_epochs, device):\n",
    "    # Optimizer, Loss Function, and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.001,\n",
    "        steps_per_epoch=int(len(train_dataloader)),\n",
    "        epochs=number_of_epochs,\n",
    "        anneal_strategy=\"linear\",\n",
    "    )\n",
    "\n",
    "    # Repeat this process for each epoch.\n",
    "    for epoch in range(number_of_epochs):\n",
    "        accumlated_loss = 0.0\n",
    "        number_correct = 0\n",
    "        prediction_total = 0\n",
    "\n",
    "        # Repeat this process for each batch in the training set.\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            # Place the input features and target labels on the device (GPU or CPU).\n",
    "            input_features, labels_of_batch = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Make the inputs uniform.\n",
    "            inputs_mean_value, inputs_standard_deviation = input_features.mean(), input_features.std()\n",
    "            input_features = (input_features - inputs_mean_value) / inputs_standard_deviation\n",
    "\n",
    "            # The parameter gradients should be set to zero.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward + forward + optimize\n",
    "            predicted_outputs = model(input_features)\n",
    "            calculated_loss = criterion(predicted_outputs, labels_of_batch)\n",
    "            calculated_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep track of your loss and accuracy statistics.\n",
    "            accumlated_loss += calculated_loss.item()\n",
    "\n",
    "            # Get the class with the highest predicted score.\n",
    "            _, batch_predicted_classid = torch.max(predicted_outputs, 1)\n",
    "            # The number of predictions that corresponded to the target label.\n",
    "            number_correct += (batch_predicted_classid == labels_of_batch).sum().item()\n",
    "            prediction_total += batch_predicted_classid.shape[0]\n",
    "\n",
    "        # At the end of the epoch, print the statistics.\n",
    "        num_batches = len(train_dataloader)\n",
    "        avg_loss = accumlated_loss / num_batches\n",
    "        accuracy = number_correct / prediction_total\n",
    "        print(f\"Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "def test_inference(model, validation_dataloader, device):\n",
    "\n",
    "    number_corrected = 0\n",
    "    prediction_total = 0\n",
    "\n",
    "    # Turn off gradient updates.\n",
    "    with torch.no_grad():\n",
    "        for data in validation_dataloader:\n",
    "            # Place the input features and target labels on the device (GPU or CPU).\n",
    "            input_features, labels_of_batch = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Make the inputs uniform.\n",
    "            inputs_mean_value, inputs_standard_deviation = input_features.mean(), input_features.std()\n",
    "            input_features = (input_features - inputs_mean_value) / inputs_standard_deviation\n",
    "\n",
    "            # Get predicted outputs\n",
    "            predicted_outputs = model(input_features)\n",
    "\n",
    "            # Get the class with the highest predicted score.\n",
    "            _, batch_predicted_classid = torch.max(predicted_outputs, 1)\n",
    "            # The number of predictions that corresponded to the target label.\n",
    "            number_corrected += (batch_predicted_classid == labels_of_batch).sum().item()\n",
    "            prediction_total += batch_predicted_classid.shape[0]\n",
    "\n",
    "    accuracy = number_corrected / prediction_total\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Total items: {prediction_total}\")\n",
    "\n",
    "def main():\n",
    "    install_libraries()\n",
    "\n",
    "    base_directory = Path.cwd() / 'UrbanSound8K'\n",
    "    dataframe = prepare_metadata(base_directory)\n",
    "    audio_path = base_directory / 'audio'\n",
    "    train_dataloader, validation_dataloader = create_data_loaders(dataframe, audio_path)\n",
    "\n",
    "    audioClassifierModel = AudioClassifier()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    audioClassifierModel = audioClassifierModel.to(device)\n",
    "\n",
    "    number_of_epochs = 2\n",
    "    train_model(audioClassifierModel, train_dataloader, number_of_epochs, device)\n",
    "    test_inference(audioClassifierModel, validation_dataloader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
